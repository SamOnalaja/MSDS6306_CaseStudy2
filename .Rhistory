knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(caret)
library(leaps)
library(MASS)
dataset <- read_csv("CaseStudy2-data.csv",
col_types = cols(EmployeeCount = col_skip(), ID = col_skip(), Over18 = col_skip(), Rand = col_skip(), StandardHours = col_skip()))
dataset$Attrition[dataset$Attrition=="Yes"] <- 1
dataset$Attrition[dataset$Attrition=="No"] <- 0
model_aic <- lm(Attrition ~ ., data = dataset)
step.model_both <- stepAIC(model_aic, direction = "both", trace = FALSE)
step.model_forward <- stepAIC(model_aic, direction = "forward", trace = FALSE)
step.model_backward <- stepAIC(model_aic, direction = "backward", trace = FALSE)
aic_both <- summary(step.model_both)
aic_forward <- summary(step.model_forward)
aic_backward <- summary(step.model_backward)
aic_both$adj.r.squared
aic_forward$adj.r.squared
aic_backward$adj.r.squared
model_aic <- lm(Attrition ~ ., data = dataset)
step.model_both <- stepAIC(model_aic, direction = "both", trace = FALSE)
step.model_forward <- stepAIC(model_aic, direction = "forward", trace = FALSE)
step.model_backward <- stepAIC(model_aic, direction = "backward", trace = FALSE)
aic_both <- summary(step.model_both)
aic_forward <- summary(step.model_forward)
aic_backward <- summary(step.model_backward)
aic_both$adj.r.squared
aic_forward$adj.r.squared
aic_backward$adj.r.squared
summary(model_aic)
step.model_both$anova
#this did pretty bad, R2 values of 0.25
model_aic <- lm(Attrition ~ ., data = dataset)
step.model_both <- stepAIC(model_aic, direction = "both", trace = FALSE)
step.model_forward <- stepAIC(model_aic, direction = "forward", trace = FALSE)
step.model_backward <- stepAIC(model_aic, direction = "backward", trace = FALSE)
aic_both <- summary(step.model_both)
aic_forward <- summary(step.model_forward)
aic_backward <- summary(step.model_backward)
aic_both$adj.r.squared
aic_forward$adj.r.squared
aic_backward$adj.r.squared
step.model_forward$anova
step.model_forward$anova
step.model_both$anova
leaps <- regsubsets(Attrition ~ ., data=dataset, nbest=10)
summary(leaps)
plot(leaps, scale="r2")
rm(leaps)
library(DAAG)
install.packages("DAAG")
library(DAAG)
cv.lm(df=dataset, model_aic, m=3)
cv.lm(df=dataset, model_aic, m=3)
cv.lm(dataset, model_aic, m=3)
View(dataset)
dataset$Attrition <- as.integer(dataset$Attrition)
View(dataset)
cv.lm(dataset, model_aic, m=3)
install.packages("bestglm")
#this did pretty bad, R2 values of 0.25
model_aic <- lm(Attrition ~ ., data = dataset)
step.model_both <- stepAIC(model_aic, direction = "both", trace = FALSE)
step.model_forward <- stepAIC(model_aic, direction = "forward", trace = FALSE)
step.model_backward <- stepAIC(model_aic, direction = "backward", trace = FALSE)
aic_both <- summary(step.model_both)
aic_forward <- summary(step.model_forward)
aic_backward <- summary(step.model_backward)
aic_both$adj.r.squared
aic_forward$adj.r.squared
aic_backward$adj.r.squared
library(bestglm)
library(bestglm)
?bestglm
pleaseWork <- bestglm(dataset, family = leaps, IC = "CV", CVArgs = "default", TopModels = 3, method = "exhaustive", intercept = TRUE)
dataset$BusinessTravel <- as.factor(dataset$BusinessTravel)
View(dataset)
levels(dataset$BusinessTravel)
dataset <- read_csv("CaseStudy2-data.csv",
col_types = cols(EmployeeCount = col_skip(), ID = col_skip(), Over18 = col_skip(), Rand = col_skip(), StandardHours = col_skip()))
dataset[sapply(dataset, is.character)] <- lapply(dataset[sapply(dataset, is.character)], as.factor)
View(dataset)
#this did pretty bad, R2 values of 0.25
model_aic <- lm(Attrition ~ ., data = dataset)
step.model_both <- stepAIC(model_aic, direction = "both", trace = FALSE)
#maybe do this
dataset$Attrition[dataset$Attrition=="Yes"] <- 1
dataset$Attrition[dataset$Attrition=="No"] <- 0
dataset$Attrition <- as.integer(dataset$Attrition)
View(dataset)
dataset <- read_csv("CaseStudy2-data.csv",
col_types = cols(EmployeeCount = col_skip(), ID = col_skip(), Over18 = col_skip(), Rand = col_skip(), StandardHours = col_skip()))
#maybe do this
dataset$Attrition[dataset$Attrition=="Yes"] <- 1
dataset$Attrition[dataset$Attrition=="No"] <- 0
dataset$Attrition <- as.integer(dataset$Attrition)
dataset[sapply(dataset, is.character)] <- lapply(dataset[sapply(dataset, is.character)], as.factor)
View(dataset)
#this did pretty bad, R2 values of 0.25
model_aic <- lm(Attrition ~ ., data = dataset)
step.model_both <- stepAIC(model_aic, direction = "both", trace = FALSE)
step.model_forward <- stepAIC(model_aic, direction = "forward", trace = FALSE)
step.model_backward <- stepAIC(model_aic, direction = "backward", trace = FALSE)
aic_both <- summary(step.model_both)
aic_forward <- summary(step.model_forward)
aic_backward <- summary(step.model_backward)
aic_both$adj.r.squared
aic_forward$adj.r.squared
aic_backward$adj.r.squared
pleaseWork <- bestglm(dataset, family = leaps, IC = "CV", CVArgs = "default", TopModels = 3, method = "exhaustive", intercept = TRUE)
pleaseWork <- bestglm(dataset, family = leaps, IC = "CV", CVArgs = "default")
pleaseWork <- bestglm(dataset, family = leaps, IC = "CV", CVArgs = "default", x = 32)
pleaseWork <- bestglm(dataset, IC="AIC")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
#Pulling in data, these will remain untouched to satisfy Professors new rules
dfTrain <- read.csv("CaseStudy2-data.csv")
dfVal <- read.csv("CaseStudy2Validation.csv")
#Dropping the following columns so we don't have to copy and paste the code dropping the columns in the analysis every time
# ID & Rand: Are arbitrary (should probably also drop EmployeeNumber? We will just see what stepwise does with it)
# Over18, StandardHours, and EmployeeCount: Only have one level
dfTrainModified <- dfTrain %>%
select(-c(Over18, ID, StandardHours, EmployeeCount, Rand)) %>%
na.omit() %>%
as.data.frame()
dfValModified <- dfVal %>%
select(-c(Over18, ID, StandardHours, EmployeeCount, Rand)) %>%
na.omit() %>%
as.data.frame()
# Force Attrition to be a factor column for the binomial analysis
dfTrainModified$Attrition <- factor(dfTrainModified$Attrition)
dfValModified$Attrition <- factor(dfValModified$Attrition)
# View the integers used
contrasts(dfTrainModified$Attrition)
# Pretending we don't have the answers in the test set
# Splitting into training and test sets
inTraining <- createDataPartition(y = dfTrainModified$Attrition, p = .60, list = FALSE)
training <- dfTrainModified[inTraining,]
testing <- dfTrainModified[-inTraining,]
# Spot checking sizes
dim(training)
dim(testing)
# Creating a model with all columns, binomial means it's a logistic regression, or regression with only two outcomes
# In our case employees either quit or they didn't
attritionModel.fit = glm(Attrition ~ ., data=training, family=binomial)
# Using Stepwise model selection to determine parameters to include in the overall formula
stepwiseGlmAnalysis <- step(attritionModel.fit, direction="both")
# Summary
summary(stepwiseGlmAnalysis)
# Summary
summary(stepwiseGlmAnalysis)
formula(stepwiseGlmAnalysis)
# Use the model found with stepwise selection to give predictions
attrition.probability = predict(glm(formula(stepwiseGlmAnalysis), data=training, family=binomial), testing, type="response")
# Make a vector of "No"
attrition.predicted = rep("No", dim(training)[1])
# Convert predicted values of greater than .5 to "Yes"
attrition.predicted[attrition.probability > .5] = "Yes"
# How did we do?
confusionMatrix(table(attrition.predicted,training$Attrition))
attritionValidate.probability = predict(glm(formula(stepwiseGlmAnalysis), data=dfValModified, family=binomial), dfValModified, type="response")
# Make a vector of "No"
attritionValidate.predicted = rep("No", dim(dfValModified)[1])
# Convert predicted values of greater than .5 to "Yes"
attritionValidate.predicted[attritionValidate.probability > .5] = "Yes"
# How did we do?
confusionMatrix(table(attritionValidate.predicted,dfValModified$Attrition))
# Creating the new dataframe that will be written
# Keeping Attrition in there to do one last test
dfPred = data.frame(dfVal[,c("ID", "Attrition")], attritionValidate.predicted)
names(dfPred) <- c("ID", "AttritionActual","AttritionPredicted")
# Making AttritionActual a factor column for the comparison
dfPred$AttritionActual <- factor(dfPred$AttritionActual)
#How did we do?
confusionMatrix(table(dfPred$AttritionPredicted, dfPred$AttritionActual))
# Write the final deliverable
write.csv(dfPred[,c("ID", "AttritionPredicted")], "Case2PredictionsPaul.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
#Import data
dfTrain <- read.csv("CaseStudy2-data.csv")
dfVal <- read.csv("CaseStudy2Validation.csv")
#Dropping the following columns so we don't have to copy and paste the code dropping the columns in the analysis every time
# ID & Rand: Are arbitrary
# Over18, StandardHours, and EmployeeCount: Only have one level
dfTrainModified <- dfTrain %>%
select(-c(Over18, ID, StandardHours, EmployeeCount, Rand)) %>%
na.omit() %>%
as.data.frame()
dfValModified <- dfVal %>%
select(-c(Over18, ID, StandardHours, EmployeeCount, Rand)) %>%
na.omit() %>%
as.data.frame()
# Force Attrition to be a factor column for the binomial analysis
dfTrainModified$Attrition <- factor(dfTrainModified$Attrition)
dfValModified$Attrition <- factor(dfValModified$Attrition)
# View the integers used
contrasts(dfTrainModified$Attrition)
# Pretending we don't have the answers in the test set
# Splitting into training and test sets
inTraining <- createDataPartition(y = dfTrainModified$Attrition, p = .60, list = FALSE)
training <- dfTrainModified[inTraining,]
testing <- dfTrainModified[-inTraining,]
# Spot checking sizes
dim(training)
dim(testing)
# Creating a model with all columns, binomial means it's a logistic regression, or regression with only two outcomes
# In our case employees either quit or they didn't
attritionModel.fit = glm(Attrition ~ ., data=training, family=binomial)
# Using Stepwise model selection to determine parameters to include in the overall formula
stepwiseGlmAnalysis <- step(attritionModel.fit, direction="both")
# Summary
summary(stepwiseGlmAnalysis)
formula(stepwiseGlmAnalysis)
# Use the model found with stepwise selection to give predictions
attrition.probability = predict(glm(formula(stepwiseGlmAnalysis), data=training, family=binomial), testing, type="response")
# Make a vector of "No"
attrition.predicted = rep("No", dim(training)[1])
# Convert predicted values of greater than .5 to "Yes"
attrition.predicted[attrition.probability > .5] = "Yes"
# How did we do?
confusionMatrix(table(attrition.predicted,training$Attrition))
attritionValidate.probability = predict(glm(formula(stepwiseGlmAnalysis), data=dfValModified, family=binomial), dfValModified, type="response")
# Make a vector of "No"
attritionValidate.predicted = rep("No", dim(dfValModified)[1])
# Convert predicted values of greater than .5 to "Yes"
attritionValidate.predicted[attritionValidate.probability > .5] = "Yes"
# How did we do?
confusionMatrix(table(attritionValidate.predicted,dfValModified$Attrition))
# Creating the new dataframe that will be written
# Keeping Attrition in there to do one last test
dfPreds = data.frame(dfVal[,c("ID", "Attrition")], attritionValidate.predicted)
names(dfPreds) <- c("ID", "AttritionActual","AttritionPredicted")
# Making AttritionActual a factor column for the comparison
dfPreds$AttritionActual <- factor(dfPreds$AttritionActual)
#How did we do?
confusionMatrix(table(dfPreds$AttritionPredicted, dfPreds$AttritionActual))
# Write the final deliverable
write.csv(dfPreds[,c("ID", "AttritionPredicted")], "Case2PredictionsAdams_Paul.csv", row.names = FALSE)
# Creating the new dataframe that will be written
# Keeping Attrition in there to do one last test
dfPreds = data.frame(dfVal[,c("ID", "Attrition")], attritionValidate.predicted)
names(dfPreds) <- c("ID", "AttritionActual","AttritionPredicted")
# Making AttritionActual a factor column for the comparison
dfPreds$AttritionActual <- factor(dfPreds$AttritionActual)
#How did we do?
confusionMatrix(table(dfPreds$AttritionPredicted, dfPreds$AttritionActual))
# Write the final deliverable
write.csv(dfPreds[,c("ID", "AttritionPredicted")], "Case2PredictionsAdams_Paul.csv", row.names = FALSE)
